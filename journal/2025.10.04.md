# Journal — 10-04-2025

## 1) What I learned (bullets, not prose)
- Raw should stay raw
- Usual data is not usually made understandable because it prioritizes efficieny. When cleaning, better to add another column representing the change rather than transforming the raw.
- Make sure to focus on framing the problems first. Normally DEs do not own the business questions, but it will be greatly helpful in creating the pipeline.
    - Business Questions are usually about making money, save money, and compliance
- Trade offs are essential and inevitable. Data has history so it usually happens that companies have existing data models already that has been edited time and time again which leads to trade offs.
- Data quality is all about creating data that people actually trusts.
- Five Dimensions of DQ: Accuracy, Completeness, Reliability, Revelevancy, and Timeliness.
  - Ingestion (raw): schema/type, categories, FKs
  - Transform (clean): null, categories, FKs
  - Mart: reasonableness, uniqueness, business rules
  - ReportingL last-updated, data health cards
- Quality checks should be done in both Clean and Mart (everywhere basically)
- Data owner have the decision making power
- Steward is the executioner of the Data owner

## 2) New vocabulary (define in your own words)
- Data Quality Contract: creating an agreement with the business on what they require in the reports (thresholds, SLA, owner)
- Logging Over Time: recording how much is ingested over time
- MTTD: Mean Time to Detect
- MTTR: Mean Time to Resolve
- Lineage: traceability of data

## 3) Data Engineering mindset applied (what principles did I use?)
- Raw stays raw
- Reproducible
- Document assumptions and decisions
- Small changes

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- Main trade off was choosing to focus on building the pipeline instead of business questions. Since we were lacking on time, for the OULAD dataset our group decided to visualize based on the data we have already on the pipeline rather than creating a clear business question first.
- Another trade off was since we are all more familiar in SQL rather than editing dbt, we did the DQ using SQL instead

## 5) Open questions (things I still don’t get)
- how to create/understand a python script for ingestion?
- how to visualize DQ when using dbt?

## 6) Next actions (small, doable steps)
- [ ] try to run the whole process by myself
- [ ] finish the DQ dashboard using SQL
- [ ] Data Camp

## 7) Artifacts & links (code, queries, dashboards)
- Group 6 Metabase OULAD: https://ftw.dataengineering.ph/dashboard/43-group-6-oulad
- Lecture: https://classroom.google.com/u/0/c/ODAyMjYwODE2ODE2/m/ODAzOTczMjczNzY3/details

---

### Mini reflection (3–5 sentences)
Creating things that will persist over time.
Data quality is king.
DEs need to work with multiple people.

Another reflection for this week is how hard it is to have good quality data. I really like what Sir Myk said about how there is no existing perfectly made data out there, but we try our best to get as close as we can because most of the time there are a lot at stake specially for the companies. Also, good quality is not just a work of data engineers, but rather a collaborative aspect of the whole process.


### BONUS: What is a meme that best describes what you feel or your learning today?

![Alt text](https://file.forms.app/sitefile/d-s-m-3.jpeg)
