# Journal — 10-11-2025

## 1) What I learned (bullets, not prose)
- Data Lifecycle: Collection > Storage > Transformation > Serving > Visualization/Decision
- Flow: Source > Transport > Landing > Staging > Modeled > Serving
- JSON works in key-value pairs. Example "id":1, "student":Ana
- YML uses syntax, indention. Uses key-value pair as well.
- uv in python allows us to isolate the changes in the directory only. For example, the dependencies can only occur in the directory
- Dataframe is essentially tabular while series is a list
- Particular project use pandas, but if you want to connect to systems use Ibis
- Web scraping/web harvesting/web data extraction is data scraping used for extracting data from website.
- How website works: There is a server containing files like home page then Google comes in asking for a page then the server shares an HTML version of the file.
- JavaScript makes the website dynamic.

## 2) New vocabulary (define in your own words)
- XML: Extensible Markup Language -  you can create various data types
- API: Application Programming Interface
- Abstraction: we do not need to know the underlying codes that make it work, like an umbrella code containing other mini codes

## 3) Data Engineering mindset applied (what principles did I use?)
- Raw stays raw
- Reproducible
- Document assumptions and decisions
- Communicate clearly
- Observe & log

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- Because our team is not yet as familiar with data checking using dbt, we opted to use SQL which is more manual and takes a bit more time
- Our team decided to split tasks per person which allowed us to work remotely but at the same time created dependencies wherein next person could not start working without the other person finishing first.

## 5) Open questions (things I still don’t get)
- How to do data checking using dbt?
- How can I print the 03-api.py in tabular form instead of JSON?
- How to convert different file formats to CSV?

## 6) Next actions (small, doable steps)
- [ ] Try using dbt for data checking
- [ ] Check out additional Data Camp materials
- [ ] Read through possible data sets for capstone

## 7) Artifacts & links (code, queries, dashboards)
- Lecture file: https://classroom.google.com/u/0/c/ODAyMjYwODE2ODE2/m/ODA0MjMzODc0NTc4/details
- Instacart Metabase: https://ftw.dataengineering.ph/dashboard/54-group-4-instacart?tab=79-dashboard
- Instacart Docu: https://github.com/jessielynlozano/FTW-DE-Groupings/blob/main/journal/Instacart.md

---

### Mini reflection (3–5 sentences)
There is no one sure way to do do things.
Building things to last.
Data will always have a certain kind of bias.

Learning about API was really interesting. I have been interested in it lately and now I finally understand how it possibly works. Basically it is kind of an umbrella code with a lot of underlying codes hidden beneath that allows us to connect one machine to the other with easier commands. 

Web Scraping was also highly interesting. I didn't know that data that from websites can be in tabular form when we check the underlying codes. It made me realize how extensive data is in our real life which of course leads to a lot of ethical concerns.

### BONUS: What is a meme that best describes what you feel or your learning today?

![Alt text](https://pbs.twimg.com/media/DRE2qIMWAAEU9Kn.jpg)
