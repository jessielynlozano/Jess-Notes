# Journal — 9-13-2025 — REAL

## 1) What I learned (bullets, not prose)
- Data engineering is all about moving data from one place to another. It is the foundation of the whole data cycle
- I think the reason Data Lake is called as such becuase much like nature it houses the raw/natural form of data while Data Warehouse is already edited in some way
- DLT as container and DBT to transform data
- Clickhouse is a sample of database
- Data industry is rapidly moving. Do not bound yourself with titles and what you just know. Always be curious and alway continue to learn

## 2) New vocabulary (define in your own words)
- DAMA Wheel: shows the various aspects of data management
- ETL: Extract Transform Load
-	Because database is expensive, we remove all the unnecessary data by Transform stage
-	ELT: Extract Load Transform
-	Because of cheaper database like cloud, it is fine now to load all data
-	Allows users to go back to raw data in case they would need it
- RCM Pipeline: Bronze is raw data, Silver is semi-structured data, and Gold is business-ready data
- RDMS – relational database management system
-	DBMS 
-	OLAP made to house the information for query (historical, analytical, then query)
-	OLTP operational database

## 3) Data Engineering mindset applied (what principles did I use?)
- Raw stays raw!

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- On one of the practice wherein we have to do our own SQL code in Clickhouse, at first I wanted to create a range for the year column similar with a pivot table. However, due to time constrictions and lack of knowledge regarding SQL coding, I decided to retain the data as is. I can't help but think there would a lot of similar cases like that in the real world of coding where you want to create something but have to trade it off for now to allow users to use the data in a timely manner.

## 5) Open questions (things I still don’t get)
- How does the Docker, Terminal, and Dbeaver actually know that they are connected? How come a code in terminal will change the current visuals in Dbeaver?
- How does the whole local setup actually work? Already started and got most of the answers in this chatGPT thread: https://chatgpt.com/share/68c8be6c-1f40-8010-98dc-4e4470bd8875
- Also connected above is how .yaml/.yml file works and how do I create my own? Do I always have to use something like that when I use dlt and/or dbt?
- If not Docker, Dbeaver, or Terminal, what are the applications that most companies use?

## 6) Next actions (small, doable steps)
- [ ] find a list of all docker commands
- [ ] finish group assigment
- [ ] finish datacamp
- [ ] possibly browse some tutorial dojo links

## 7) Artifacts & links (code, queries, dashboards)
- https://dataengineering.ph/

---

### Mini reflection (3–5 sentences)
I think one of the surprising part for is the sheer amount of work and tools it take to deliver good quality data to our end users. I used to do operations and data is a big thing in a day-to-day basis but we mainly just use Microsoft query and pivot table. Even just from those steps, we get so much insights already, imagine if one can create an edible data that spans year or even decades of business transactions, consumer profile, etc. That would be like a whole new world for strategy and decision-making.


### BONUS: What is a meme that best describes what you feel or your learning today?

![Alt text](../assets/BlogBanner9-1708194729796.png.avif)
